![App Screenshot](assets/chunkwise_UI_2.png)
![App Screenshot](assets/chunkwise_UI_1.png)

# ğŸ“š URL-Based Q&A Application using LangChain, FAISS, Groq, and Streamlit

This project is an intelligent research assistant that enables users to input news article URLs and ask contextual questions. It processes content using **LangChain's RAG (Retrieval-Augmented Generation)** pipeline and answers questions by referencing specific article chunks. Designed for **finance analysts, researchers**, and **information seekers**, this tool overcomes traditional LLM limitations.

---

## ğŸ”§ Technologies Used

| Component            | Technology                             |
|---------------------|-----------------------------------------|
| Frontend UI         | Streamlit                               |
| LLM Backend         | Groq (LLaMA3 API)                        |
| Vector Embeddings   | SentenceTransformer (`all-mpnet-base-v2`) |
| Vector Database     | FAISS (Facebook AI Similarity Search)   |
| Document Splitter   | RecursiveCharacterTextSplitter (LangChain) |
| Programming Language| Python                                  |

---

## ğŸš¨ Why Not Just Use ChatGPT?

While ChatGPT and other LLMs are powerful, they fall short for real-time equity or news-based research due to:

- âŒ Tedious copy-pasting of long articles  
- âŒ No unified knowledge base for cross-article context  
- âŒ Token/word limits (typically ~3000 words)  
- âŒ Higher cost per token when feeding full documents  

---

## ğŸ¯ What This Tool Solves

âœ… Automates document fetching from URLs  
âœ… Splits large texts into manageable **"chunks"**  
âœ… Embeds and stores them in a searchable vector DB  
âœ… Retrieves only relevant chunks for each question  
âœ… Answers queries using a smart, token-efficient QA chain  
âœ… Cites the **source URLs** in answers  

---

## ğŸ§  Core Concept: RAG (Retrieval-Augmented Generation)

This project uses **RAG** to power question-answering:

> Instead of feeding the entire document to the LLM, **RAG retrieves only relevant "chunks"** from stored embeddings and uses them as context for generating answers.

**Think of it like:**

> ğŸ“– â€œOpen book exam where you first look up relevant sections before answering the question.â€

---

## ğŸ”„ Pipeline Flow

